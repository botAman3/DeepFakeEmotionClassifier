Deepfake Audio Detection using Vision Transformers
This project implements a deep learning model to distinguish between original, authentic audio and synthetically generated deepfake audio from various neural codecs.

The primary approach transforms audio signals into visual representations (Mel-spectrograms) and leverages a pre-trained Vision Transformer (ViT) to perform binary classification. This method treats audio classification as an image recognition task, capitalizing on state-of-the-art computer vision architectures.

Features
State-of-the-Art Architecture: Utilizes a pre-trained Vision Transformer (vit_b_16) on Mel-spectrograms for high-accuracy classification.

Modular & Clean Code: The project is structured into logical modules for configuration, data handling, model definition, training, and inference, making it easy to understand and extend.

Robust Data Handling: The data pipeline is designed to handle large datasets, including robust error handling for corrupted or silent audio files.

Comprehensive Evaluation: The training script evaluates the model using multiple metrics, including Accuracy, F1-Score, and Equal Error Rate (EER).

Ready-to-Use Inference: Includes a simple command-line script to run predictions on a single audio file using the trained model.

Project Structure
The code is organized into the following Python files for clarity and maintainability:

.
├── data/
│   ├── original/
│   │   ├── audio_001.wav
│   │   └── ...
│   ├── encoder1/
│   │   ├── audio_001.wav
│   │   └── ...
│   ├── encoder2/
│   │   └── ...
│   └── encoder3/
│       └── ...
├── config.py           # All hyperparameters and settings
├── dataset.py          # PyTorch Dataset for audio-to-spectrogram conversion
├── model.py            # Vision Transformer model definition
├── engine.py           # Core training and evaluation loops
├── train.py            # Main script to start the training process
├── inference.py        # Script to run predictions on a single audio file
└── vit_classifier.pth  # Saved model weights (generated after training)

Setup and Installation
1. Clone the Repository

git clone <your-repository-url>
cd <your-repository-name>

2. Create a Python Environment

It is highly recommended to use a virtual environment.

python -m venv env
source env/bin/activate  # On Windows, use `env\Scripts\activate`

3. Install Dependencies

Install all the required libraries using pip:

pip install torch torchaudio torchvision scikit-learn numpy tqdm seaborn matplotlib

4. Dataset Setup

Place your audio files in the data/ directory following the structure specified above. The model expects a directory of original audio and one or more directories of corresponding fake audio generated by different codecs. The paths can be easily adjusted in config.py.

Usage
The project has two main modes: training and inference.

1. Training the Model

To train the model on your dataset, run the train.py script. It will load the configuration, build the datasets, and start the training process.

python train.py

The script will log the progress for each epoch and save the final model weights to vit_classifier.pth upon completion.

2. Running Inference

Once the model is trained and vit_classifier.pth has been created, you can use inference.py to predict whether a single audio file is real or fake.

python inference.py --file /path/to/your/audio.wav

Example Output:

--- Inference Result ---
File: sample_deepfake.wav
Prediction: Fake
Confidence (Fake Score): 0.9876

Model Architecture Explained
Audio to Spectrogram: Each audio waveform is converted into a Mel-spectrogram. This process transforms the one-dimensional audio signal into a two-dimensional "image" where the x-axis represents time, the y-axis represents frequency, and the pixel intensity represents the amplitude of a frequency at a particular time.

Vision Transformer (ViT): The generated spectrogram image is then fed into a Vision Transformer. Unlike traditional Convolutional Neural Networks (CNNs), ViT splits the image into a sequence of fixed-size patches, flattens them, and processes them using a standard Transformer encoder, similar to those used in NLP models like BERT. This allows the model to capture global, long-range relationships between different parts of the spectrogram, which is highly effective for identifying subtle, widespread artifacts common in deepfakes.

Transfer Learning: We use a ViT model pre-trained on the massive ImageNet dataset. We freeze the weights of the pre-trained model and add a new, small, trainable classifier head. This technique, known as transfer learning, allows us to leverage the powerful visual feature extraction capabilities learned from millions of images and apply them to our spectrograms.

